test_tennis <- data.frame(
Outlook = factor(c("Sunny", "Rain", "Rain")),
Temperature = factor(c("Mild", "Cool", "Hot")),
Humidity = factor(c("Normal", "High", "Normal")),
Wind = factor(c("Strong", "Weak", "Strong")),
PlayTennis = factor(c("Yes", "Yes", "No"))
)
k = 5000
# Now we process the data so it works in C++ using the utils function
test_tennis = num_matrix_from_df(test_tennis)
# We extract the target variable and features from the processed dataset
X <- test_tennis[, 1:4] # Features: Outlook, Temperature, Humidity, Wind
y <- test_tennis[, 5]   # Target: PlayTennis
# gini_part_C ----
gini_result <- gini_part_C(y)
cat("Gini Impurity for PlayTennis using C++:", gini_result, "\n")
### Test script for C code
# library appendix ----
library(rpart)
library(rpart.plot)
library(grid)
library(gridExtra)
library(ggplot2)
library(microbenchmark)
library(Rcpp)
library(dplyr)
## function appendix ----
# Function for preprocessing the data into C++ format.
source("utils.R")
knitr::opts_chunk$set(echo = TRUE)
factor_columns <- c("dorm", "banos", "tipo.casa", "inter.exter",
"ascensor", "estado", "comercial", "casco.historico", "M.30")
summary(data[factor_columns])
summary(data_processed[factor_columns])
## Preprocess  -----------------------------------------------------------------
preprocess = function(data,
predictors){
# Log transform of price per sqm
data$precio.house.m2 <- log(data$precio.house.m2)
# radius
# Load required library
library(geosphere)
# Central point (Puerta del Sol)
center <- c(-3.7038, 40.4168)
# Calculate distances and add a new column
data$radius <- distHaversine(
matrix(c(data$longitud, data$latitud), ncol = 2),
matrix(rep(center, nrow(data)), ncol = 2, byrow = TRUE)
) / 1000  # Convert meters to kilometers
# Turn categorical columns to factors
factor_columns <- c("barrio", "distrito", "banos", "dorm", "tipo.casa", "inter.exter",
"ascensor", "estado", "comercial", "casco.historico", "M.30")
data[factor_columns] <- lapply(data[factor_columns], as.factor)
# group categories via manual evaluation
# Dorm
data <- data %>%
mutate(
dorm = case_when(
dorm %in% c("0", "1") ~ "0-1",
dorm %in% c("4", "5", "6", "7") ~ "+4",
TRUE ~ as.character(dorm)
)
)
data$dorm <- factor(data$dorm, levels = unique(data$dorm))
# Banos
data <- data %>%
mutate(
banos = case_when(
banos %in% c("3", "4", "5", "7") ~ "+3",
TRUE ~ as.character(banos)
)
)
data$banos <- factor(data$banos, levels = unique(data$banos))
# tipo.casa
data <- data %>%
mutate(
tipo.casa = case_when(
tipo.casa %in% c("atico", "estudio", "Otros") ~ "Atico/Estudio",
TRUE ~ as.character(tipo.casa)
)
)
data$tipo.casa <- factor(data$tipo.casa, levels = unique(data$tipo.casa))
# estado
data <- data %>%
mutate(
estado = case_when(
estado %in% c("a_reformar", "reg,-mal") ~ "Low_standards",
estado %in% c("excelente", "nuevo-semin,", "reformado") ~ "High_standards",
estado %in% c("buen_estado", "segunda_mano") ~ "Mid_standards",
)
)
data$estado <- factor(data$estado, levels = unique(data$estado))
data <- data %>%
mutate(
distrito = case_when(
# South Districts
distrito %in% c("carabanchel", "puente_vallecas", "usera", "vallecas", "villaverde") ~ "South",
# Central Districts
distrito %in% c("arganzuela", "centro", "chamberi", "retiro", "salamanca") ~ "Centro",
# North Districts
distrito %in% c("barajas", "chamartin", "fuencarral", "hortaleza", "tetuan") ~ "North",
# West Districts
distrito %in% c("moncloa", "latina") ~ "West",
# East Districts
distrito %in% c("vallecas", "moratalaz", "vicalvaro", "san_blas", "ciudad_lineal") ~ "East",
# Default to original values if no match
TRUE ~ as.character(distrito)
)
)
data$distrito <- factor(data$distrito, levels = unique(data$distrito))
# Function to normalize multiple variables
normalize_variables <- function(variables) {
for (var in variables) {
data[[var]] <- (data[[var]] - mean(data[[var]], na.rm = TRUE)) / sd(data[[var]], na.rm = TRUE)
}
return(data)
}
# Generate the formula automatically
num_id <- sapply(data, is.numeric)
num_vars <- names(data)[num_id] %>% setdiff(c("precio.house.m2", "radius"))
data = normalize_variables(num_vars)
# Eliminar columnas no deseadas
data <- subset(data, select=predictors)
return(data)
}
data_processed = preprocess(data)
# Necessary Libraries ---------------------------------------------------------
if (!require(readxl)) install.packages("readxl")
if (!require(leaps)) install.packages("leaps")
if (!require(caret)) install.packages("caret")
if (!require(groupdata2)) install.packages("groupdata2")
if (!require(car)) install.packages("car")
if (!require(dplyr)) install.packages("dplyr")
if (!require(Metrics)) install.packages("Metrics")
library(readxl)
library(leaps)
library(caret)
library(groupdata2)
library(car)
library(dplyr)
library(Metrics)
data_processed = preprocess(data)
summary(data_processed[factor_columns])
data[factor_columns] <- lapply(data[factor_columns], as.factor)
summary(data[factor_columns])
summary(data_processed[factor_columns])
summary(data[factor_columns])
names(data)
summary(data$Pobl.0_14_div_Poblac.Total)
sum(unique(data$Pobl.0_14_div_Poblac.Total)
)
length(unique(data$Pobl.0_14_div_Poblac.Total)
)
# Part 2.----
## library appendix ----
library(rpart)
library(rpart.plot)
library(grid)
library(gridExtra)
library(ggplot2)
library(microbenchmark)
## function appendix ----
# Function to save plots
save_plot <- function(plot, filename) {
ggsave(
filename = paste0(filename, ".jpg"),
plot = plot,
width = 8, height = 6, units = "in", dpi = 300
)
}
# Function for calculatin trees of nÂºdepth
source("utils.R")
library(readxl)
library(leaps)
library(caret)
library(groupdata2)
library(car)
library(dplyr)
library(mgcv)
library(MASS)
library(geosphere)
## Check Multicoliniearity -----------------------------------------------------
check_multicollinearity <- function(model, data) {
# Identify numerical and categorical variables
predictors <- labels(terms(model)) # Variables used in the model
data <- data[predictors]
num_id <- sapply(data, is.numeric)
num_vars <- names(data)[num_id]
cat_vars <- names(data)[!num_id]
# Model Matrix
X <- data[,num_vars]
R <- cor(X)
# Calculate condition number using kappa
condition_number <- kappa(R, exact = TRUE)
# Calculating VIF values using the car package
vif_values <- tryCatch({
vif(model)
}, error = function(e) {
warning("Could not calculate VIF due to collinearity issues.")
return(NA)
})
# Generate warnings if there are significant problems
if (condition_number > 30) {
warning("High condition number detected, indicating potential multicollinearity issues.")
}
if (any(vif_values > 10, na.rm = TRUE)) {
warning("VIF values greater than 10 detected, indicating potential multicollinearity issues.")
}
# Give out diagnostics
cat("=== Multicollinearity Diagnostics ===\n")
if (is.na(condition_number)) {
cat("Serious issues detected in the condition number.\n")
} else {
cat("Condition Number:", round(condition_number, 2), "\n")
}
cat("VIF Values:\n")
if (all(is.na(vif_values))) {
cat("VIF values could not be calculated due to collinearity.\n")
} else {
print(vif_values)
}
# Return results
return(list(
condition_number = condition_number,
vif_values = vif_values
))
}
### K-fold CV ------------------------------------------------------------------
k_fold <- function(data, k, cat_vars = c("tipo.casa"), obj_var = "y") {
# Create a k-fold partition with balanced cat_vars and which
# tries to minimize similar values in obj_var
folded_data <- fold(data,
k = k,
cat_col = cat_vars,
num_col = obj_var)
# It adds a new variable, .folds, which assigns a value 1 to k to each
# instance, dividing them by folds
# Return the new dataset
return(folded_data)
}
#### WARNING --- IN PROGRESS
### Loop to find a comprehensively balanced seed for the k-fold
seed <- 1
mix <- 1000000
k = 4
Tot_table <- list()
print(seed) # Up until 2000 ---> 416 is the best (k=4) 44 is the best seed (k=10)
i = 416
set.seed(i)
folded_data <- fold(data_train,
k = k,
cat_col = "tipo.casa",
num_col = "y")
k = 4
mix <- 1000000
k = 4
n <- 736
set.seed(i)
folded_data <- fold(data_train,
k = k,
cat_col = "tipo.casa",
num_col = "y")
print(seed) # Up until 2000 ---> 416 is the best (k=4) 44 is the best seed (k=10)
for (j in 1:k){
print(paste(j, "- Fold   ==================================================="))
for(var in cat_vars){
print(paste(var,"----------------------------------------------------------"))
print(table(data_train[[as.name(var)]][which(folded_data$.folds == j)])/ll)
print(Tot_table[[as.name(var)]])
}
}
# Identify numerical and categorical variables
predictors <- labels(terms(model)) # Variables used in the model
data <- data[predictors]
num_id <- sapply(data, is.numeric)
num_vars <- names(data)[num_id]
cat_vars <- names(data)[!num_id]
k_fold <- function(data, k=4, cat_vars = c("tipo.casa"), obj_var = "y") {
# Set the previously studied best seed (balance-wise)
set.seed(416)
# Create a k-fold partition with balanced cat_vars and which
# tries to minimize similar values in obj_var
folded_data <- fold(data,
k = k,
cat_col = cat_vars,
num_col = obj_var)
# It adds a new variable, .folds, which assigns a value 1 to k to each
# instance, dividing them by folds
# Return the new dataset
return(folded_data)
}
fit_linear_model = function(formula, data_train){
model = lm(formula,data = data_train)
return(model)
}
k_fold_cv_linear_model <- function(model_formula,
data_train,
k=4){
cat("=== Running k_fold Cross Validation === \\")
# Create the K-fold partition
folded_data <- k_fold(data_train,k)$.folds
# Initialize a vector to store each fold's rsme
cv_rmse <- numeric(k)
# Initialize a vector to store each fold's Rsq_adj
cv_rsq_adj <- numeric(k)
for (i in 1:k){
# Create the fold's test/train split
temp_train <- data_train[which(folded_data!=i),]
temp_test <- data_train[which(folded_data==i),]
# Fit the model and make predictions
temp_model <- fit_linear_model(model_formula, temp_train)
temp_predictions <- predict(temp_model, newdata = temp_test)
## Calculate error metrics and store them
# rsq adj
n_train = nrow(temp_train)
n_test = nrow(temp_test)
num_predictors = length(coefficients(temp_model))
SSE = sum(((temp_test$y) - (temp_predictions))^2)
temp_mean = mean((temp_test$y))
SSR = sum((temp_mean -(temp_predictions))^2)
SST = SSE + SSR
cv_rsq_adj[i] = 1 - (SSE/(n_test-num_predictors))/(SST/(n_test-1))
# rmse
cv_rmse[i] <- sqrt(SSE/n_test)
SSE = 0
SST = 0
}
# Return the vector with rmse for each k-fold
return(list(cv_rmse=cv_rmse,
cv_rsq_adj=cv_rsq_adj))
}
data <- read_excel("Data/data_train.xlsx")
# radius
# Central point (Puerta del Sol)
center <- c(-3.7038, 40.4168)
# Calculate distances and add a new column
data$radius <- distHaversine(
matrix(c(data$longitud, data$latitud), ncol = 2),
matrix(rep(center, nrow(data)), ncol = 2, byrow = TRUE)
) / 1000  # Convert meters to kilometers
# distrito
data$distrito[data$distrito %in% c("carabanchel", "puente_vallecas", "usera","vallecas","villaverde")] = "south"
data$distrito[data$distrito %in% c("arganzuela", "centro", "chamberi","retiro","salamanca")] = "centro"
data$distrito[data$distrito %in% c("barajas", "chamartin", "fuencarral", "hortaleza", "tetuan")] = "north"
data$distrito[data$distrito %in% c("moncloa","latina")] = "west"
data$distrito[data$distrito %in% c("vallecas","moratalaz","vicalvaro","san_blas","ciudad_lineal")] = "east"
# dorm
data$dorm[data$dorm %in% c("0","1")] = "0&1"
data$dorm[data$dorm %in% c("3","4")] = "3&4"
data$dorm[data$dorm %in% c("5","6","7","8","9","10")] = "5+"
#banos
data$banos[data$banos %in% c("3","4","5","6","7","8")] = "3+"
# type
data$tipo.casa[data$tipo.casa %in% c("Otros","piso")] = "piso"
data$tipo.casa[data$tipo.casa %in% c("chalet","duplex")] = "chalet+duplex"
data$tipo.casa[data$tipo.casa %in% c("atico","estudio")] = "atico+estudio"
# state
data$estado[data$estado %in% c("excelente","nuevo-semin,","reformado")] = "bueno"
data$estado[data$estado %in% c("buen_estado","segunda_mano")] = "medio"
data$estado[data$estado %in% c("a_reformar","reg,-mal")] = "malo"
# normalize latitude and longitude
data$longitud <- (data$longitud - mean(data$longitud))/sd(data$longitud)
data$latitud <- (data$latitud - mean(data$latitud))/sd(data$latitud)
data$train_indices <- NULL
data$log.precio.house.m2 <- log(data$precio.house.m2)
data$precio.house.m2 <- NULL
data$barrio <- NULL
data$cod_barrio <- NULL
data$cod_distrito <- NULL
data$log.sup.util <- log(data$sup.util)
data$sup.util <- NULL
data$sup.const <- NULL
factor_columns <- c("distrito", "dorm", "banos", "tipo.casa", "inter.exter",
"ascensor", "estado", "comercial", "casco.historico", "M.30")
data[factor_columns] <- lapply(data[factor_columns], as.factor)
num_id <- sapply(data, is.numeric)
num_vars <- names(data)[num_id]
num_vars
cat_vars <- names(data)[!num_id]
cat_vars
data_train <- data
data_train$y <- data_train$log.precio.house.m2
data_train$log.precio.house.m2 <- NULL
num_id <- sapply(data_train, is.numeric)
num_vars <- setdiff(names(data_train)[num_id],"y")
predictors <- c("ref.hip.zona", "antig", "Poca_limp", "PM10", "Pobl.0_14_div_Poblac.Total" ,   "PoblJubilada_div_Poblac.Total", "Inmigrantes.porc", "Pocas_zonas")
cat_vars = factor_columns
# Create a fucntion to automatically normalize the numerical vars
normalize = function(row){
row = (row - mean(row))/sd(row)
return(row)
}
data_train[setdiff(num_vars, "radius")] = apply(data_train[setdiff(num_vars, "radius")], 2, normalize)
#### No interactions
lm_formula <- as.formula(
paste("y ~", paste(c(num_vars, cat_vars), collapse = " + "))
)
lm_model = lm(lm_formula,data = data_train)
summary(lm_model)
check_multicollinearity(lm_model, data = data_train)
k_fold_cv_linear_model(lm_formula, data_train)
knitr::opts_chunk$set(echo = TRUE)
k_fold <- function(data, k=4, cat_vars = c("tipo.casa"), obj_var = "y") {
# Set the previously studied best seed (balance-wise)
set.seed(416)
# Create a k-fold partition with balanced cat_vars and which
# tries to minimize similar values in obj_var
folded_data <- fold(data,
k = k,
cat_col = cat_vars,
num_col = obj_var)
# It adds a new variable, .folds, which assigns a value 1 to k to each
# instance, dividing them by folds
# Return the new dataset
return(folded_data)
}
cat_vars
cat_vars = c("distrito", "banos", "dorm", "tipo.casa", "inter.exter",
"ascensor", "estado", "comercial", "casco.historico", "M.30")
names(data)
k_data = k_fold(data, 4, cat_vars, "log.precio.house.m2")
k_data = k_fold(data_train, 4, cat_vars, "log.precio.house.m2")
k_data = k_fold(data, 4, cat_vars, "log.precio.house.m2")
# Create the K-fold partition
folded_data <- k_fold(data_train,k)$.folds
folded_data
# Initialize a vector to store each fold's rsme
cv_rmse <- numeric(k)
# Initialize a vector to store each fold's Rsq_adj
cv_rsq_adj <- numeric(k)
for (i in 1:k){
# Create the fold's test/train split
temp_train <- data_train[which(folded_data!=i),]
temp_test <- data_train[which(folded_data==i),]
# Fit the model and make predictions
temp_model <- fit_linear_model(model_formula, temp_train)
temp_predictions <- predict(temp_model, newdata = temp_test)
## Calculate error metrics and store them
# rsq adj
n_train = nrow(temp_train)
n_test = nrow(temp_test)
num_predictors = length(coefficients(temp_model))
SSE = sum(((temp_test$y) - (temp_predictions))^2)
temp_mean = mean((temp_test$y))
SSR = sum((temp_mean -(temp_predictions))^2)
SST = SSE + SSR
cv_rsq_adj[i] = 1 - (SSE/(n_test-num_predictors))/(SST/(n_test-1))
# rmse
cv_rmse[i] <- sqrt(SSE/n_test)
SSE = 0
SST = 0
}
# Create the fold's test/train split
temp_train <- data_train[which(folded_data!=i),]
temp_test <- data_train[which(folded_data==i),]
# Para cargar la data ir a nicos thingis 222
i = 1
temp_test <- data_train[which(folded_data==i),]
summary(temp_test)
summary(temp_test[,cat_vars])
summary(temp_test[,cat_vars])/n
summary(temp_test[,cat_vars])
# Calculate proportions for each categorical variable
proportions <- lapply(cat_vars, function(var) {
counts <- table(temp_test[[var]])             # Count occurrences
props <- counts / sum(counts)                 # Calculate proportions
data.frame(Count = counts, Proportion = props)  # Combine counts and proportions
})
# Assign names to the list
names(proportions) <- cat_vars
proportions[1]
temp_test = data
# Calculate proportions for each categorical variable
proportions <- lapply(cat_vars, function(var) {
counts <- table(temp_test[[var]])             # Count occurrences
props <- counts / sum(counts)                 # Calculate proportions
data.frame(Count = counts, Proportion = props)  # Combine counts and proportions
})
# Assign names to the list
names(proportions) <- cat_vars
proportions[1]
# Initialize an empty list to store proportions for each fold and the global dataset
all_proportions <- list()
# Function to calculate proportions for a dataset
calculate_proportions <- function(data, cat_vars) {
lapply(cat_vars, function(var) {
counts <- table(data[[var]])  # Count occurrences
props <- counts / sum(counts)  # Calculate proportions
data.frame(Category = names(counts), Count = as.numeric(counts), Proportion = round(props, 4))
})
}
# Add global proportions to the list
all_proportions$Global <- calculate_proportions(data, cat_vars)
# Add proportions for each fold to the list
for (i in unique(folded_data)) {
temp_test <- data_train[which(folded_data == i), ]
all_proportions[[paste("Fold", i)]] <- calculate_proportions(temp_test, cat_vars)
}
# Combine proportions into a single table for each variable
combined_tables <- lapply(cat_vars, function(var) {
# Extract the data for the variable from all proportions
tables <- lapply(all_proportions, function(prop_list) prop_list[[var]])
# Combine the tables, aligning on the 'Category' column
combined <- Reduce(function(x, y) merge(x, y, by = "Category", all = TRUE), tables)
# Rename columns for clarity
colnames(combined) <- c("Category",
"Global_Count", "Global_Proportion",
paste0(rep(c("Fold", "Proportion"), length(unique(folded_data))), "_", rep(1:length(unique(folded_data)), each = 2)))
# Replace NAs with zeros for missing categories
combined[is.na(combined)] <- 0
return(combined)
})
