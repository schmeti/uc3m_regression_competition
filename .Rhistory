left_data <- y[c(1,2)]
right_data <- y[c(3,4)]
gini_impurity_result <- gini_impurity_C(left_data, right_data)
cat("Weighted Gini Impurity In Perfect purity case:", gini_impurity_result, "\n")
best_split_resoult <- best_split_optional_C(X,y)
print(best_split_resoult)
decision_stump_optional <- fit_decision_stump_optional_C(X, y)
print(decision_stump_optional)
# Preprocess data for rpart
data_rpart <- cbind(X, y = factor(y, labels = c("No", "Yes")))
# microbenchmark
microbenchmark(
gini_cpp = fit_decision_stump_optional_C(as.matrix(X), as.integer(y)),  # C++ function
gini_R = fit_decision_stump_optional_R(as.matrix(X), as.integer(y)),   # R custom function
gini_rpart = rpart(y ~ ., data = as.data.frame(data_rpart),
control = rpart.control(cp = 0, maxdepth = 1)),
times = k,
unit = "microseconds"
)
# Case in which no gain is achived through one division
no_gain <- data.frame(
Outlook = factor(c("Sunny", "Sunny", "Rain", "Rain")),
Temperature = factor(c("Cold", "Cold", "Hot", "Hot")),
Humidity = factor(c("Normal", "Normal", "High", "High")),
Wind = factor(c("Strong", "Strong", "Weak", "Weak")),
PlayTennis = factor(c("No", "Yes", "Yes", "No"))
) %>% num_matrix_from_df()
X <- no_gain[, 1:4] # Features: Outlook, Temperature, Humidity, Wind
y <- no_gain[, 5]   # Target: PlayTennis
left_data <- y[c(1,3)]
right_data <- y[c(2,4)]
gini_impurity_result <- gini_impurity_C(left_data, right_data)
cat("Weighted Gini Impurity In Perfect purity case:", gini_impurity_result, "\n")
decision_stump_optional <- fit_decision_stump_optional_C(X, y)
print(decision_stump_optional)
# Preprocess data for rpart
data_rpart <- cbind(X, y = factor(y, labels = c("No", "Yes")))
# microbenchmark
microbenchmark(
gini_cpp = fit_decision_stump_optional_C(as.matrix(X), as.integer(y)),  # C++ function
gini_R = fit_decision_stump_optional_R(as.matrix(X), as.integer(y)),   # R custom function
gini_rpart = rpart(y ~ ., data = as.data.frame(data_rpart),
control = rpart.control(cp = 0, maxdepth = 1)),
times = k,
unit = "microseconds"
)
# original dataset ----
play_tennis <- read.csv('play_tennis.csv', stringsAsFactors = TRUE)
mat_play_tennis <- num_matrix_from_df(play_tennis)
# We extract the target variable and features from the processed dataset
X <- test_tennis[, 1:4] # Features: Outlook, Temperature, Humidity, Wind
y <- test_tennis[, 5]   # Target: PlayTennis
fit_decision_stump_optional_C(X, y)
fit_decision_stump_optional_R(X, y)
# original dataset ----
play_tennis <- read.csv('play_tennis.csv', stringsAsFactors = TRUE)
mat_play_tennis <- num_matrix_from_df(play_tennis)
# We extract the target variable and features from the processed dataset
X <- test_tennis[, 1:4] # Features: Outlook, Temperature, Humidity, Wind
y <- test_tennis[, 5]   # Target: PlayTennis
fit_decision_stump_optional_C(X, y)
fit_decision_stump_optional_R(X, y)
test_tennis <- data.frame(
Outlook = factor(c("Sunny", "Rain", "Rain")),
Temperature = factor(c("Mild", "Cool", "Mild")),
Humidity = factor(c("Normal", "High", "Normal")),
Wind = factor(c("Strong", "Weak", "Strong")),
PlayTennis = factor(c("Yes", "Yes", "No"))
)
k = 5000
# Now we process the data so it works in C++ using the utils function
test_tennis = num_matrix_from_df(test_tennis)
# We extract the target variable and features from the processed dataset
X <- test_tennis[, 1:4] # Features: Outlook, Temperature, Humidity, Wind
y <- test_tennis[, 5]   # Target: PlayTennis
fit_decision_stump_optional_C(X, y)
fit_decision_stump_optional_R(X, y)
# original dataset ----
play_tennis <- read.csv('play_tennis.csv', stringsAsFactors = TRUE)
mat_play_tennis <- num_matrix_from_df(play_tennis)
# We extract the target variable and features from the processed dataset
X <- mat_play_tennis[, 1:4] # Features: Outlook, Temperature, Humidity, Wind
y <- mat_play_tennis[, 5]   # Target: PlayTennis
fit_decision_stump_optional_C(X, y)
fit_decision_stump_optional_R(X, y)
View(rpart_depth)
rpart(1)
rpart_depth(1)
X
head(X)
head(play_tennis)
play_tennis %>% filter(Outlook == "Overcast") %>% head()
play_tennis %>% filter(Outlook == "Overcast") %>% summary()
X[, best_feature] == best_value
X[, 0] == 0
X[, 0+1] == 0
X[, 0+1] == 0
left_indices <- X[, best_feature] == best_value
left_indices <- X[, best_feature+1] == best_value
left_indices
best_value
left_indices <- X[, 1] == 0
left_indices
y[left_indices]
right_indices <- X[, 1] != 0
right_indices
left <- y[left_indices]
right <- y[right_indices]
right
# Compute the class probabilities:
# Left branch class
sum_0 <- sum(left == 0) # Count the instances in the left branch with class 0
sum_1 <- sum(left == 1) # Count the instances in the left branch with class 1
# Conditional statement to choose the majoritary class
if (sum_0 == sum_1){
# If the classes are equirepresented, indicate it by assigning an impossible
# value of 0,5
left_class <- 0.5
} else {
# If there is a majoritary class, calculate it using the which.max()
# function in R but noting that the indexes in R start in 1
left_class <- which.max(c(sum_0,sum_1)) - 1
}
# Right branch class
sum_0 <- sum(right == 0)
sum_1 <- sum(right == 1)
if (sum_0 == sum_1){
right_class <- 0.5
} else {
right_class <- which.max(c(sum_0,sum_1)) - 1
}
#        'y': A boolean vector corresponding to the instance's classification
#             (0 = No, 1 = Yes).
# OUTPUT: 'output': A 3-dimensional vector storing the info. of the best split:
#                   The feature used in it (integer >= 1),
#                   The value used in the feature (integer >= 1),
#                   The corresponding Gini impurity measure (0<= double <= 0.5),
#                   The majoritary class in the left branch of the split and
#                   The majoritary class in the right branch of the split
#                   (0, 1 or 0.5 if the classes are equirepresented)
################################################################
best_split_optional_R <- function(X, y) {
# Initialize the value 'best_gini', which stores the lowest gini index among
# the studied possible splits. We initialize it as 1, because it is trivially
# updated within the first iteration of the algorithm since gini <= 0.5
best_gini <- 1.0
# Initialize the value 'best_feature', which stores the feature used in the
# split with lowest gini index among the studied ones. We initialize it as -1,
# because it is trivially updated within the first iteration of the algorithm
best_feature <- -1
# Calculate the number of features measured in the dataset, which is the number
# of columns in X
n_features <- ncol(X)
# Initialize a loop iterating through the columns/features of X
for (feature in 1:n_features) {
# Obtain all the different values/classes in the selected feature,
# which would be from 0 to the max value of the column
values <- 0:max(X[, feature])
# Iterate through the different values of the selected feature
for (value in values) {
# Split the data: In left_indices, store the indices of those
# instances which take the selected value of the previously selected feature.
# In right_indices, store the rest of the individuals
# Note that, since in our dataset, the most different values a feature can
# take is 3, this way we are considering all possible splits made through
# the selected feature
left_indices <- X[, feature] == value
right_indices <- X[, feature] != value
# Store the values of the objective variable (which is boolean)
left <- y[left_indices]
right <- y[right_indices]
# Calculate, using the previously defined gini_impurity_R(), the weighted
# average Gini impurity measure of the split
gini <- gini_impurity_R(left, right)
# If the Gini measure is better than the best one yet (which means that
# the split is better than the best one among the ones already studied),
# overwrite the variables describing the previous best split.
if (gini < best_gini) {
best_gini <- gini       # Overwrite the best Gini index yet
best_feature <- feature # Overwrite the best feature yet
best_value <- value     # Overwrite the best value of teh feature yet
}
# Else, don't overwrite the values, since the split is worse than the
# best one yet
# Keep iterating through the values of the feature
}
# Keep iterating through the features
}
# Compute the best split. It is more efficient to compute it at the end and
# obtain the majority classes rather than calculating them in each iteration
left_indices <- X[, best_feature] == best_value
right_indices <- X[, best_feature] != best_value
left <- y[left_indices]
right <- y[right_indices]
# Compute the class probabilities:
# Left branch class
sum_0 <- sum(left == 0) # Count the instances in the left branch with class 0
sum_1 <- sum(left == 1) # Count the instances in the left branch with class 1
# Conditional statement to choose the majoritary class
if (sum_0 == sum_1){
# If the classes are equirepresented, indicate it by assigning an impossible
# value of 0,5
left_class <- 0.5
} else {
# If there is a majoritary class, calculate it using the which.max()
# function in R but noting that the indexes in R start in 1
left_class <- which.max(c(sum_0,sum_1)) - 1
}
# Right branch class
sum_0 <- sum(right == 0)
sum_1 <- sum(right == 1)
if (sum_0 == sum_1){
right_class <- 0.5
} else {
right_class <- which.max(c(sum_0,sum_1)) - 1
}
# Create a list of values to return which describe the best split
output <- list(
best_feature = best_feature -1 , # Best feature (-1 to be coherent with c++ indexing)
best_value = best_value,         # Best value of the feature to split with (-1 to be coherent with c++ indexing)
best_gini = best_gini,           # Gini index of the best split
left_class = left_class,         # Majoritary class in the left branch of the split
right_class = right_class        # Majoritary class in the right branch of the split
)
# Return the constructed list
output
}
fit_decision_stump_optional_C(X, y)
fit_decision_stump_optional_R(X, y)
# original dataset ----
play_tennis <- read.csv('play_tennis.csv', stringsAsFactors = TRUE)
mat_play_tennis <- num_matrix_from_df(play_tennis)
# We extract the target variable and features from the processed dataset
X <- mat_play_tennis[, 1:4] # Features: Outlook, Temperature, Humidity, Wind
y <- mat_play_tennis[, 5]   # Target: PlayTennis
fit_decision_stump_optional_C(X, y)
fit_decision_stump_optional_R(X, y)
# Case in which no gain is achived through one division
no_gain <- data.frame(
Outlook = factor(c("Sunny", "Sunny", "Rain", "Rain")),
Temperature = factor(c("Cold", "Cold", "Hot", "Hot")),
Humidity = factor(c("Normal", "Normal", "High", "High")),
Wind = factor(c("Strong", "Strong", "Weak", "Weak")),
PlayTennis = factor(c("No", "Yes", "Yes", "No"))
) %>% num_matrix_from_df()
X <- no_gain[, 1:4] # Features: Outlook, Temperature, Humidity, Wind
y <- no_gain[, 5]   # Target: PlayTennis
left_data <- y[c(1,3)]
right_data <- y[c(2,4)]
gini_impurity_result <- gini_impurity_C(left_data, right_data)
cat("Weighted Gini Impurity In Perfect purity case:", gini_impurity_result, "\n")
decision_stump_optional <- fit_decision_stump_optional_C(X, y)
print(decision_stump_optional)
# Preprocess data for rpart
data_rpart <- cbind(X, y = factor(y, labels = c("No", "Yes")))
# microbenchmark
microbenchmark(
gini_cpp = fit_decision_stump_optional_C(as.matrix(X), as.integer(y)),  # C++ function
gini_R = fit_decision_stump_optional_R(as.matrix(X), as.integer(y)),   # R custom function
gini_rpart = rpart(y ~ ., data = as.data.frame(data_rpart),
control = rpart.control(cp = 0, maxdepth = 1)),
times = k,
unit = "microseconds"
)
# microbenchmark
microbenchmark(
gini_cpp = fit_decision_stump_optional_C(as.matrix(X), as.integer(y)),  # C++ function
gini_R = fit_decision_stump_optional_R(as.matrix(X), as.integer(y)),   # R custom function
gini_rpart = rpart(y ~ ., data = as.data.frame(data_rpart),
control = rpart.control(cp = 0, maxdepth = 1)),
times = k,
unit = "microseconds"
)
knitr::opts_chunk$set(echo = TRUE)
eta_squared_sorted
library(effectsize) # For eta squared computation
data = read_excel("Data/data_train.xlsx")
setwd("C:/GitHub/uc3m_regression_competition")
library(effectsize) # For eta squared computation
data = read_excel("Data/data_train.xlsx")
knitr::opts_chunk$set(echo = TRUE)
source("modelling_pipeline.R")
library(effectsize) # For eta squared computation
data = read_excel("Data/data_train.xlsx")
data_processed = preprocess(data)
num_id <- sapply(data_processed, is.numeric)
num_vars <- names(data_processed)[num_id]
num_vars
cat_vars <- names(data_processed)[!num_id]
cat_vars
# Compute the correlations with "precio.house.m2"
correlations <- sapply(data_processed[,num_id],
function(x) cor(data_processed$y, x,
use = "complete.obs"))
correlations[order(abs(correlations),decreasing = T)[2:8]]
# Compute Eta Squared for the categorical variables
eta_squared_results <- sapply(data_processed[, cat_vars], function(cat_var) {
# Fit a simple ANOVA model
model <- aov(data_processed$y ~ as.factor(cat_var))
# Calculate Eta Squared
eta_squared(model, partial = FALSE)$Eta2
})
# Sort Eta Squared values in decreasing order and display the most relevant ones
eta_squared_sorted <- eta_squared_results[order(eta_squared_results, decreasing = TRUE)]
eta_squared_sorted
eta_squared_sorted
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
data_train <- read_excel("Data/data_train.xlsx")
#install.packages(c("ggplot2", "sf", "dplyr"))
# Load libraries
library(ggplot2)
library(sf)
library(dplyr)
library(GGally)
library(knitr)
library(kableExtra)
library(geosphere)
library(gridExtra)
colnames(data_train)
# turn identifiers into strings to prevent errors
# data_train[, c("train_indices", "cod_barrio", "cod_distrito", "ref.hip.zona ", "casco.historico", "M.30","comercial")] <- lapply(data_train[, c("train_indices", "cod_barrio", "cod_distrito", "ref.hip.zona", "casco.historico", "M.30","comercial")], as.character)
data_train[, c("train_indices", "cod_barrio", "cod_distrito", "ref.hip.zona")] = lapply(data_train[, c("train_indices", "cod_barrio", "cod_distrito", "ref.hip.zona")], as.character)
cat.var = c("dorm", "banos", "tipo.casa", "inter.exter", "ascensor", "comercial",
"casco.historico", "M.30")
data_train[, c(cat.var, "barrio", "distrito")] =
lapply(data_train[, c(cat.var, "barrio", "distrito")], as.factor)
num_id <- sapply(data_train, is.numeric)
num_vars <- names(data_train)[num_id]
unique_counts <- data.frame(
Column = names(data_train),
Unique_Values = sapply(data_train, function(x) length(unique(x)))
)
unique_counts
summary(data_train)
plot_hist = function(data_train, type = "both", n, m) {
# Configure subplots
par(mfrow = c(n, m),mar = c(10, 4, 4, 1))
# Loop through columns
for (col in colnames(data_train)) {
# plot numeric columns
if (is.numeric(data_train[[col]]) && (type == "numeric" || type == "both")) {
# Create histograms
hist(data_train[[col]],
main = paste(col),
xlab = "",
col = "dodgerblue4")
# plot non-numeric columns
} else if (!is.numeric(data_train[[col]]) && (type == "categoric"
|| type == "both")) {
# Adjust x-axis label arrangement for specific cols
barplot(table(data_train[[col]]),
main = paste(col),
xlab = "",
col = "dodgerblue",
las = 2)
}
}
# Reset the plotting layout
par(mfrow = c(1, 1))
}
# Numeric hist
plot_hist(data = data_train ,type="numeric", n=3, m=2)
# Numeric hist
plot_hist(data = data_train ,type="categoric", n=3, m=2)
# Compute correlation matrix
# Convert Categorical Variables to Numeric Using Label Encoding
data_encoded <- data_train %>%
mutate(across(where(is.character), ~ as.numeric(factor(.))))
numeric_columns <- data_encoded[, sapply(data_encoded, is.numeric)]
# Compute the correlation matrix
correlations <- cor(numeric_columns)
# Check if 'precio.house.m2' exists
if ("precio.house.m2" %in% colnames(correlations)) {
# Extract the row corresponding to 'precio.house.m2'
price_per_sqm_corr <- correlations["precio.house.m2", ]
# Convert to data frame for plotting
price_per_sqm_corr_df <- data.frame(
Variable = names(price_per_sqm_corr),
Correlation = price_per_sqm_corr
)
# Plot the correlations
library(ggplot2)
ggplot(price_per_sqm_corr_df, aes(x = reorder(Variable, Correlation), y = Correlation)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() + # Flip coordinates for easier readability
theme_minimal() +
labs(
title = "Correlation with price_per_sqm",
x = "Variable",
y = "Correlation"
)
} else {
stop("precio.house.m2 not found in correlation matrix.")
}
library(corrplot)
# Plot
corrplot(correlations, method = "circle", addCoef.col = "black", type = "upper",
order = "hclust", tl.col = "black", tl.srt = 45)
# Install and load necessary packages
library(reshape2)
library(ggplot2)
library(GGally) # This is required for ggpairs
# Compute correlation matrix
numeric_column_names <- names(data_train)[sapply(data_train, is.numeric)]
corr_mat <- round(cor(data_train[, numeric_column_names]), 2)
melted_corr_mat <- melt(corr_mat)
# Plot correlation heatmap
ggplot(data = melted_corr_mat, aes(x = Var2, y = Var1, fill = value)) +
geom_tile() +
geom_text(aes(label = value), color = "black", size = 4)+
scale_fill_gradient(low = "white", high = "red") +
theme_minimal()
library(ggcorrplot)
p <- ggcorrplot(corr_mat, title = "Matriz de correlaciones",ggtheme = ggplot2::theme_minimal(),
hc.order = TRUE) +
theme(axis.text.x = element_text(size = 7)) +
theme(axis.text.y = element_text(size = 7))
ggsave("Matriz de correlaciones.jpg", plot = p, width = 10, height = 10, units = "in", dpi = 300)
# Pairplot using ggpairs
ggpairs(data_train[, numeric_column_names])
cramersV(data_train$dorm, data_train$banos)
hist(data$sup.util)
hist(log(data$sup.util))
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#FFB6C1")
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#FFB6C1")+
theme_bw()
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#EEA2AD")+
theme_bw()
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#FFAEB9")+
theme_bw()
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#FFAEB9", binwidth = 0.1)+
theme_bw()
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#FFAEB9", binwidth = 10)+
theme_bw()
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#FFAEB9", binwidth = 30)+
theme_bw()
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#FFAEB9", binwidth = 30, alpha = 0.9)+
theme_bw()
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#FFAEB9", binwidth = 30, alpha = 0.8)+
theme_bw()
ggplot(data = data, aes(x=sup.util))+
geom_histogram(color = "black", fill = "#FFAEB9",
binwidth = 30, alpha = 0.8)+
theme_bw()+
theme(y = "Frequency")
ggplot(data = data, aes(x = sup.util)) +
geom_histogram(color = "black", fill = "#FFAEB9",
binwidth = 30, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
ggplot(data = data, aes(x = sup.util)) +
geom_histogram(color = "black", fill = "#FFAEB9",
binwidth = 20, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
ggplot(data = data, aes(x = log(sup.util))) +
geom_histogram(color = "black", fill = "#FFAEB9",
binwidth = 20, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
ggplot(data = data, aes(x = log(sup.util))) +
geom_histogram(color = "black", fill = "#FFAEB9",
binwidth = 0.1, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
ggplot(data = data, aes(x = log(sup.util))) +
geom_histogram(color = "black", fill = "#A2CD5A",
binwidth = 0.1, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
ggplot(data = data, aes(x = log(sup.util))) +
geom_histogram(color = "black", fill = "#698B69",
binwidth = 0.1, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
ggplot(data = data, aes(x = log(sup.util))) +
geom_histogram(color = "black", fill = "darkseagreen3",
binwidth = 0.1, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
ggplot(data = data, aes(x = log(sup.util))) +
geom_histogram(color = "black", fill = "#8FBC8F",
binwidth = 0.1, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
ggplot(data = data, aes(x = log(sup.util))) +
geom_histogram(color = "black", fill = "#8FBC8F",
binwidth = 0.2, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
ggplot(data = data, aes(x = log(sup.util))) +
geom_histogram(color = "black", fill = "#8FBC8F",
binwidth = 0.15, alpha = 0.8) +
theme_bw() +
labs(y = "Frequency")
summary(data_processed$tipo.casa)
summary(data_processed$tipo.casa)/n*100
summary(data_processed$tipo.casa)/n
638+62+26
638/726
62/726
36/726
